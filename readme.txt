Setup:
	Dependencies:
		Python3.5
		Keras: https://github.com/keras-team/keras
		TensorFlow: https://www.tensorflow.org/
		YOLO: https://github.com/experiencor/keras-yolo3
		Opencv
Usage:
	Train model
	Put the taninning image to the `testData/images/`, and put the XML file which record the coordinates of the obejct to the `testData/annots/, then run `python train.py -c config.json` to tain model,
 	I have pre-trained a model `animation.h5` which can detect demonhunter and sunwell

	Make annimation
	Open build_animation.py, and modify the text, then run `python build_animation.py` to generate animation, all output animations will be save to `outputVideo/`, there are 4 animations, character_detection.mp4 and Fullvideo_detection.mp4 is generated by YOLO object detection, character_subtraction.mp4 is generated by implementing background subtraction function, output.mp4 is final output video.  
	
	Video Dataset
	All video data is saved to `dataVideo/`, include different background video and character action video, currently, there are only 3 different background videos and 2 action videos,so user can not provide a text which is related to stand or move action and snowland, badland and marbleland background.



Demo Video:
	there are 2 demo videos, the run.mp4 is generated according to `Demonhunter is running on the marbleland from the right of sunwell.` the stand.mp4 is generated according to `Demonhunter is standing on the badland from the right of sunwell.`


